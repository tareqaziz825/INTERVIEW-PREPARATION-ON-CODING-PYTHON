{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ad534a1",
   "metadata": {},
   "source": [
    "I have carefully scanned your resume (Mohammod Tareq Aziz, BRAC University, ML/DL & Computer Vision focus). Based strictly on your resume and your target (Samsung R&D Institute Bangladesh â€“ Software/AI role), here are the **5 most common interview questions with strong, customized answers tailored for you.**\n",
    "\n",
    "These are written in **natural interview speaking style**, not robotic.\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ â€œIntroduce Yourself.â€\n",
    "\n",
    "â€œMy name is Mohammod Tareq Aziz. I recently completed my B.Sc. in Computer Science from BRAC University with a CGPA of 3.35. My academic focus has been on machine learning, deep learning, and computer vision.\n",
    "\n",
    "For my thesis, I worked on a context-aware zero-shot video anomaly detection system where I combined TimeSformer-based spatiotemporal modeling with CLIP-based semantic alignment. The system achieved 84.5% ROC-AUC on the UCF-Crime dataset and outperformed existing pure zero-shot baselines.\n",
    "\n",
    "Previously, I also worked on a machine learning research project analyzing blue-light blocking glasses using spectrophotometric data, where KNN achieved an RÂ² score of 0.914.\n",
    "\n",
    "I enjoy solving real-world problems using data-driven approaches, and Iâ€™m particularly interested in building scalable and efficient AI systems. Iâ€™m now looking to start my professional journey where I can contribute to impactful R&D work while continuously learning.â€\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ â€œWhy Samsung R&D Bangladesh?â€\n",
    "\n",
    "â€œI am particularly interested in Samsung R&D Bangladesh because it is one of the few companies in the country that truly focuses on research-driven software development. \n",
    "\n",
    "I also appreciate Samsungâ€™s global impact. Working here would allow me to contribute to products used worldwide while learning from experienced engineers in a structured R&D environment.\n",
    "\n",
    "Additionally, Samsung values innovation, optimization, and real-world deployment â€” which connects well with my thesis work on both advanced research models and lightweight deployment systems.â€\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ â€œExplain Your Thesis in Simple Terms.â€\n",
    "\n",
    "â€œMy thesis focuses on detecting abnormal activities in surveillance videos without training the model on abnormal examples. Normally, anomaly detection systems require labeled abnormal data, but anomalies are rare and diverse.\n",
    "\n",
    "So I built a zero-shot anomaly detection system that learns only from normal behavior. It uses a transformer-based spatiotemporal model to understand motion patterns and CLIP-based semantic alignment to match video features with contextual descriptions.\n",
    "\n",
    "If the video deviates from learned normal motion patterns or doesnâ€™t align semantically with the scene context, the system flags it as an anomaly.\n",
    "\n",
    "This approach improves generalization and achieved 84.5% ROC-AUC on the UCF-Crime dataset.â€\n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ â€œWhat Was Your Biggest Technical Challenge?â€\n",
    "\n",
    "â€œIn my thesis, one major challenge was balancing semantic alignment and temporal prediction. Initially, the model produced high false positives because semantic alignment was overly sensitive to small deviations.\n",
    "\n",
    "To solve this, I implemented a context-gating mechanism and carefully balanced the alignment loss and predictive loss using weighted combination. I also conducted ablation studies to identify which components improved performance the most.â€\n",
    "\n",
    "---\n",
    "\n",
    "### 5ï¸âƒ£ â€œWhy Should We Hire You?â€\n",
    "\n",
    "â€œI believe Iâ€™m a good fit because I combine research understanding with practical implementation skills. I have hands-on experience with PyTorch, Transformers, CNNs, and ML pipelines. Iâ€™ve worked on both high-level research models and lightweight deployment systems.\n",
    "\n",
    "Beyond technical skills, Iâ€™m comfortable reading research papers, conducting experiments, and optimizing models â€” which is important in an R&D environment.\n",
    "\n",
    "Iâ€™m highly motivated, eager to learn, and ready to contribute to real-world engineering problems while continuously improving myself.â€\n",
    "\n",
    "---\n",
    "\n",
    "### â€œWhat Are Your Strengths and Weaknesses?â€\n",
    "\n",
    "â€œOne of my strengths is persistence in solving complex technical problems. During my thesis, I repeatedly refined the architecture and ran multiple experiments until achieving stable performance improvements.â€\n",
    "\n",
    "### âœ… Weakness (Safe & Professional)\n",
    "\n",
    "â€œI sometimes spend too much time optimizing details. However, Iâ€™ve learned to balance optimization with deadlines by prioritizing impact first.â€\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  Final Advice for You\n",
    "\n",
    "When answering:\n",
    "\n",
    "- Keep answers 60â€“90 seconds\n",
    "- Speak calmly\n",
    "- Donâ€™t rush technical terms\n",
    "- Maintain structure (Problem â†’ Action â†’ Result)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14568a90",
   "metadata": {},
   "source": [
    "Below are **Samsung R&Dâ€“style interview questions with strong, structured answers** based strictly on:\n",
    "\n",
    "- Your **CSE 400 Thesis** â†’ Context-Aware Zero-Shot Anomaly Detection (CLIP + TimeSformer + CPC)\n",
    "- Your **CSE 424 Project** â†’ Spatiotemporal Anomaly Detection with Knowledge Distillation\n",
    "\n",
    "These are designed for **deep technical interviews**, not basic viva.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¥ PART 1: SYSTEM DESIGN & ARCHITECTURE QUESTIONS\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q1: You worked on two anomaly detection systems. What is the fundamental difference between them?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The key difference lies in learning paradigm and deployment goal.\n",
    "\n",
    "### CSE 400 Thesis:\n",
    "\n",
    "- Zero-Shot Learning\n",
    "- Uses semantic alignment via **CLIP**\n",
    "- Spatiotemporal modeling using **TimeSformer**\n",
    "- Detects unseen anomalies\n",
    "- Research-focused, generalization-driven\n",
    "\n",
    "### CSE 424 Project:\n",
    "\n",
    "- Supervised anomaly detection\n",
    "- Uses 3D CNN like **I3D**\n",
    "- Teacherâ€“Student Knowledge Distillation\n",
    "- Deployment-focused (lightweight)\n",
    "\n",
    "So:\n",
    "\n",
    "- Thesis â†’ Generalization problem\n",
    "- Project â†’ Efficiency & deployment problem\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q2: If Samsung wants real-time deployment on edge devices, which system would you choose?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "I would choose the **CSE 424 student model**.\n",
    "\n",
    "Reason:\n",
    "\n",
    "- Lightweight architecture\n",
    "- Reduced inference time\n",
    "- No transformer overhead\n",
    "- Lower memory footprint\n",
    "\n",
    "However,\n",
    "If generalization to unseen events is critical,\n",
    "I would combine:\n",
    "\n",
    "- Thesis semantic alignment\n",
    "- Project distillation framework\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q3: Why did you choose transformer in thesis but CNN in project?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Because objectives were different.\n",
    "\n",
    "In thesis:\n",
    "\n",
    "- Long-range temporal dependency modeling required\n",
    "- Transformer attention captures global relationships\n",
    "- Important for context-aware modeling\n",
    "\n",
    "In project:\n",
    "\n",
    "- Efficiency prioritized\n",
    "- 3D CNN more computationally stable\n",
    "- Easier to distill\n",
    "\n",
    "Transformers are powerful but expensive.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¥ PART 2: DEEP MODEL UNDERSTANDING\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q4: Explain why CLIP helps in zero-shot anomaly detection.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "**CLIP** provides:\n",
    "\n",
    "- Shared vision-text embedding space\n",
    "- Rich semantic understanding from pretraining\n",
    "- Ability to match video features with scene descriptions\n",
    "\n",
    "Instead of training anomaly classes,\n",
    "we check semantic misalignment.\n",
    "\n",
    "If video embedding deviates from contextual text embedding,\n",
    "it indicates anomaly.\n",
    "\n",
    "This enables:\n",
    "\n",
    "- Detection of unseen behaviors\n",
    "- Concept-level reasoning\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q5: What is the role of Contrastive Predictive Coding in your thesis?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "CPC enforces temporal consistency.\n",
    "\n",
    "It:\n",
    "\n",
    "- Predicts future latent representations\n",
    "- Learns normal motion patterns\n",
    "- Penalizes unexpected transitions\n",
    "\n",
    "So anomaly detection happens via:\n",
    "\n",
    "- Temporal deviation\n",
    "- Semantic misalignment\n",
    "\n",
    "Dual detection mechanism increases robustness.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q6: In distillation, why not train the student directly on labels?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Because:\n",
    "\n",
    "Teacher soft outputs contain:\n",
    "\n",
    "- Dark knowledge\n",
    "- Inter-class similarity information\n",
    "- Smoother probability distributions\n",
    "\n",
    "Direct hard labels:\n",
    "\n",
    "- Lose relational information\n",
    "- Cause overfitting\n",
    "\n",
    "Distillation transfers richer supervision.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¥ PART 3: OPTIMIZATION & TRADE-OFFS\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q7: What are computational bottlenecks in your thesis model?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Major bottlenecks:\n",
    "\n",
    "1. Transformer self-attention (O(nÂ²))\n",
    "2. CLIP embedding extraction\n",
    "3. Dual loss computation\n",
    "\n",
    "TimeSformer especially expensive for long sequences.\n",
    "\n",
    "Optimization options:\n",
    "\n",
    "- Reduce frame sampling\n",
    "- Use sparse attention\n",
    "- Distill transformer into smaller model\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q8: If memory is limited to 4GB GPU, what would you modify?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "I would:\n",
    "\n",
    "- Reduce clip length\n",
    "- Use gradient checkpointing\n",
    "- Freeze CLIP encoder\n",
    "- Reduce embedding dimension\n",
    "- Replace TimeSformer with lighter 3D CNN\n",
    "\n",
    "Trade-off:\n",
    "Slight performance drop but feasible deployment.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¥ PART 4: FAILURE ANALYSIS\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q9: When would your thesis model fail?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Failure cases:\n",
    "\n",
    "1. Incorrect context description\n",
    "2. Subtle anomalies similar to normal behavior\n",
    "3. Domain shift (different camera angle)\n",
    "4. Poor lighting\n",
    "\n",
    "Since semantic alignment depends on correct context,\n",
    "wrong metadata can increase false positives.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q10: What if teacher model is biased in your project?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Student inherits bias.\n",
    "\n",
    "Distillation propagates:\n",
    "\n",
    "- Teacher errors\n",
    "- Teacher misclassification patterns\n",
    "\n",
    "Solution:\n",
    "\n",
    "- Ensure strong teacher training\n",
    "- Use validation filtering\n",
    "- Possibly ensemble teachers\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¥ PART 5: ADVANCED ENGINEERING QUESTIONS\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q11: How would you integrate both systems into a single product?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Hybrid approach:\n",
    "\n",
    "1. Use thesis model for:\n",
    "   - Semantic zero-shot detection\n",
    "\n",
    "2. Use distilled lightweight model for:\n",
    "   - Real-time anomaly scoring\n",
    "\n",
    "3. Combine scores with weighted fusion\n",
    "\n",
    "Result:\n",
    "\n",
    "- Efficient\n",
    "- Generalizable\n",
    "- Robust\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q12: How would you reduce false positives?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Strategies:\n",
    "\n",
    "- Temporal smoothing\n",
    "- Adaptive threshold\n",
    "- Context gating refinement\n",
    "- Post-processing with moving average\n",
    "- Multi-scale feature fusion\n",
    "\n",
    "False positives common in anomaly detection.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¥ PART 6: RESEARCH-LEVEL QUESTIONS\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q13: Why is anomaly detection highly imbalanced?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Because:\n",
    "\n",
    "Normal events >> abnormal events.\n",
    "\n",
    "Consequences:\n",
    "\n",
    "- Accuracy meaningless\n",
    "- Model biased toward normal\n",
    "- Requires ROC-AUC, PR-AUC\n",
    "\n",
    "Need careful threshold tuning.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q14: Explain difference between reconstruction-based and prediction-based anomaly detection.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Reconstruction-based:\n",
    "\n",
    "- Autoencoder\n",
    "- High reconstruction error â†’ anomaly\n",
    "\n",
    "Prediction-based:\n",
    "\n",
    "- Predict future frame/feature\n",
    "- High prediction error â†’ anomaly\n",
    "\n",
    "Your thesis:\n",
    "\n",
    "- Predictive + semantic alignment\n",
    "\n",
    "Your project:\n",
    "\n",
    "- Supervised + distillation\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¥ PART 7: SAMSUNG R&D PRACTICAL THINKING\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q15: If Samsung wants anomaly detection on smart TVs or IoT cameras, what is your strategy?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "1. Use lightweight distilled student model\n",
    "2. Quantize model (INT8)\n",
    "3. Use sliding window inference\n",
    "4. Send anomaly event to server\n",
    "5. Cloud-based semantic validation\n",
    "\n",
    "Edge + cloud hybrid system.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q16: How would you measure real-time capability?\n",
    "\n",
    "Metrics:\n",
    "\n",
    "- FPS (Frames per second)\n",
    "- Latency per clip\n",
    "- GPU memory usage\n",
    "- Throughput under load\n",
    "\n",
    "Real-time usually â‰¥ 25 FPS.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¥ PART 8: CRITICAL THINKING\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q17: Why not use pure transformer in your project?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Because:\n",
    "\n",
    "- High memory cost\n",
    "- Harder to deploy\n",
    "- Distillation from transformer complex\n",
    "- CNN sufficient for moderate performance\n",
    "\n",
    "Engineering trade-off.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q18: If given 6 months more, what would you improve?\n",
    "\n",
    "For thesis:\n",
    "\n",
    "- Automatic context generation\n",
    "- Multi-modal input (audio + video)\n",
    "\n",
    "For project:\n",
    "\n",
    "- Self-supervised pretraining\n",
    "- Transformer distillation\n",
    "- Hardware-aware pruning\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ§  FINAL INTERVIEW STRATEGY\n",
    "\n",
    "In Samsung R&D style interview, they will test:\n",
    "\n",
    "- Deep model intuition\n",
    "- Trade-off reasoning\n",
    "- System optimization thinking\n",
    "- Deployment feasibility\n",
    "- Failure analysis capability\n",
    "\n",
    "You must:\n",
    "\n",
    "- Speak clearly\n",
    "- Explain architecture logically\n",
    "- Justify design decisions\n",
    "- Discuss limitations honestly\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f998f52",
   "metadata": {},
   "source": [
    "Below are **customized viva/interview questions with structured answers** strictly based on your **CSE 400 Thesis:\n",
    "â€œContext-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling.â€**\n",
    "\n",
    "These are designed for **deep technical defense**, especially for research-based interviews (Samsung R&D / ML roles).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 1: CORE RESEARCH UNDERSTANDING\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q1: What is the main problem your thesis solves?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Our thesis addresses the problem of **video anomaly detection (VAD) in surveillance systems**, particularly focusing on:\n",
    "\n",
    "- Detecting anomalies **without seeing abnormal samples during training**\n",
    "- Handling **context variation** (same behavior can be normal in one scene but abnormal in another)\n",
    "- Generalizing to **unseen anomalies** (Zero-Shot Learning scenario)\n",
    "\n",
    "Traditional models:\n",
    "\n",
    "- Depend heavily on labeled anomaly data\n",
    "- Lack context awareness\n",
    "- Fail when encountering novel anomalies\n",
    "\n",
    "Our solution:\n",
    "A **context-aware zero-shot anomaly detection framework** combining:\n",
    "\n",
    "- **OpenAIâ€™s CLIP**\n",
    "- **TimeSformer**\n",
    "- **Contrastive Predictive Coding (CPC)**\n",
    "- Context-gating mechanism\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q2: Why is anomaly detection considered a Zero-Shot Learning problem?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Because:\n",
    "\n",
    "- Anomalies are rare and diverse\n",
    "- Impossible to collect all abnormal scenarios\n",
    "- The model must detect **unseen abnormal patterns**\n",
    "\n",
    "We train only on **normal data**.\n",
    "\n",
    "At test time:\n",
    "\n",
    "- Any deviation from learned normal spatiotemporal patterns\n",
    "- Or misalignment with semantic context\n",
    "  â†’ is flagged as anomaly.\n",
    "\n",
    "This makes it a **true Zero-Shot Anomaly Detection (ZSAD)** system.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 2: ARCHITECTURE & MODEL DESIGN\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q3: Explain your complete architecture pipeline.\n",
    "\n",
    "**Answer (Structured):**\n",
    "\n",
    "Our architecture consists of four major components:\n",
    "\n",
    "### 1ï¸âƒ£ Spatiotemporal Encoder\n",
    "\n",
    "We use **TimeSformer**\n",
    "\n",
    "- Extracts spatial + temporal features\n",
    "- Uses divided space-time attention\n",
    "- Captures long-range dependencies\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Predictive Module (DPC-RNN)\n",
    "\n",
    "- Based on Contrastive Predictive Coding\n",
    "- Learns to predict future latent representations\n",
    "- If prediction error is high â†’ anomaly\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ CLIP-Based Semantic Stream\n",
    "\n",
    "We use **CLIP** text encoder:\n",
    "\n",
    "- Converts scene description into semantic embedding\n",
    "- Aligns video embeddings with text using InfoNCE loss\n",
    "- Enables semantic zero-shot detection\n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ Context-Conditioning Network\n",
    "\n",
    "- Learns scene-specific context vector\n",
    "- Uses learnable Î² parameter\n",
    "- Controls how much context influences text embedding\n",
    "- Reduces false positives\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q4: Why did you choose TimeSformer instead of CNN or LSTM?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Reasons:\n",
    "\n",
    "1. CNN â†’ local receptive field only\n",
    "2. LSTM â†’ limited long-range temporal modeling\n",
    "3. TimeSformer â†’ global self-attention\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Captures spatial and temporal dependencies jointly\n",
    "- Better modeling of complex interactions\n",
    "- Transformer-based attention scales well\n",
    "\n",
    "Especially important in crowded surveillance scenes.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q5: Why integrate CLIP?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "CLIP provides:\n",
    "\n",
    "- Pre-trained semantic knowledge\n",
    "- Shared vision-language embedding space\n",
    "- Zero-shot generalization capability\n",
    "\n",
    "Instead of training anomaly classes,\n",
    "we align:\n",
    "Video embedding â†” Scene description\n",
    "\n",
    "If misaligned â†’ anomaly.\n",
    "\n",
    "This enables:\n",
    "\n",
    "- Concept-level anomaly detection\n",
    "- Generalization to unseen behaviors\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 3: LOSS FUNCTION & TRAINING\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q6: What loss functions did you use?\n",
    "\n",
    "We use two losses:\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ Alignment Loss (InfoNCE)\n",
    "\n",
    "[\n",
    "L_{align} = - \\log \\frac{\\exp(sim(v_i, t_i)/\\tau)}{\\sum_k \\exp(sim(v_i, t_k)/\\tau)}\n",
    "]\n",
    "\n",
    "Purpose:\n",
    "\n",
    "- Pull correct video-text pairs together\n",
    "- Push mismatched pairs apart\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Predictive Loss (CPC)\n",
    "\n",
    "[\n",
    "L_{pred} = - \\log \\frac{\\exp(f(c_i)^T z_{i+1})}{\\sum_k \\exp(f(c_i)^T z_k)}\n",
    "]\n",
    "\n",
    "Purpose:\n",
    "\n",
    "- Predict future representation\n",
    "- Learn temporal consistency\n",
    "\n",
    "---\n",
    "\n",
    "### Final Loss:\n",
    "\n",
    "[\n",
    "L_{total} = \\alpha L_{align} + (1 - \\alpha) L_{pred}\n",
    "]\n",
    "\n",
    "Where Î± = 0.5 typically.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q7: Why combine predictive and contrastive learning?\n",
    "\n",
    "Because:\n",
    "\n",
    "- Contrastive learning â†’ semantic alignment\n",
    "- Predictive modeling â†’ temporal regularity\n",
    "- Combined â†’ robust anomaly detection\n",
    "\n",
    "One detects context mismatch,\n",
    "other detects temporal deviation.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 4: DATASET & EVALUATION\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q8: What dataset did you use?\n",
    "\n",
    "We evaluated on:\n",
    "\n",
    "- **UCF-Crime Dataset**\n",
    "\n",
    "Why:\n",
    "\n",
    "- Large-scale real-world surveillance dataset\n",
    "- Contains diverse anomaly types\n",
    "- Standard benchmark for VAD\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q9: What evaluation metrics did you use?\n",
    "\n",
    "- ROC-AUC\n",
    "- PR-AUC\n",
    "- mAP\n",
    "- Detection Delay\n",
    "\n",
    "Why ROC-AUC?\n",
    "Because anomaly detection is highly imbalanced.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q10: How did your model perform?\n",
    "\n",
    "Results:\n",
    "\n",
    "- ROC-AUC: 84.5%\n",
    "- PR-AUC: 72.3%\n",
    "- mAP: 62.5%\n",
    "- Detection delay: 0.45s\n",
    "\n",
    "Compared to:\n",
    "\n",
    "- AnomalyCLIP\n",
    "- Flashback\n",
    "- ViT-I3D\n",
    "\n",
    "Our model:\n",
    "\n",
    "- Best among pure zero-shot approaches\n",
    "- Balanced precision & generalization\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 5: ABLATION STUDY (VERY IMPORTANT FOR VIVA)\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q11: What did your ablation study prove?\n",
    "\n",
    "Variants:\n",
    "\n",
    "B1 â†’ Baseline\n",
    "B5 â†’ Final model\n",
    "\n",
    "Performance improved from:\n",
    "\n",
    "- ROC-AUC 46.2% â†’ 84.5%\n",
    "- PR-AUC 22.3% â†’ 72.3%\n",
    "\n",
    "Major improvements came from:\n",
    "\n",
    "- Residual MLP projection\n",
    "- LN-Gate (Î² residual)\n",
    "\n",
    "Conclusion:\n",
    "Each component significantly contributed.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 6: CONTRIBUTIONS\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q12: What are your main contributions?\n",
    "\n",
    "1ï¸âƒ£ True Zero-Shot Anomaly Detection\n",
    "2ï¸âƒ£ Joint loss (Alignment + Predictive)\n",
    "3ï¸âƒ£ Context-aware textual embedding adaptation\n",
    "4ï¸âƒ£ Reduced false positives using LN-Gate\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 7: LIMITATIONS\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q13: What are limitations of your model?\n",
    "\n",
    "- Depends on correct context metadata\n",
    "- Computationally expensive\n",
    "- Subtle anomalies remain challenging\n",
    "- Real-time deployment needs optimization\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 8: FUTURE WORK\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q14: Future improvement?\n",
    "\n",
    "- Automatic context discovery\n",
    "- Natural language anomaly explanation\n",
    "- Lightweight model for edge deployment\n",
    "- Improve efficiency\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 9: DEEP TECHNICAL GRILLING QUESTIONS\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q15: Why freeze CLIP encoder?\n",
    "\n",
    "To:\n",
    "\n",
    "- Preserve pre-trained semantic knowledge\n",
    "- Avoid catastrophic forgetting\n",
    "- Reduce training cost\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q16: Why residual MLP projection?\n",
    "\n",
    "To:\n",
    "\n",
    "- Align dimensions to CLIP space\n",
    "- Stabilize gradient flow\n",
    "- Improve representation refinement\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q17: Why LN-Gate helps?\n",
    "\n",
    "It:\n",
    "\n",
    "- Controls context influence\n",
    "- Prevents over-conditioning\n",
    "- Improves stability\n",
    "- Reduced false positives significantly\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f788efc",
   "metadata": {},
   "source": [
    "Below are **customized viva/interview questions with structured answers strictly based on your CSE 424 Project Paper (IEEE format)**.\n",
    "\n",
    "Your project focuses on a **Video Anomaly Detection system using Spatiotemporal Deep Learning with Knowledge Distillation for lightweight deployment**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 1: PROJECT OVERVIEW\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q1: What problem does your CSE 424 project solve?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Our project addresses the challenge of **real-time video anomaly detection in surveillance systems**, particularly:\n",
    "\n",
    "- Detecting unusual events automatically\n",
    "- Reducing dependency on manual monitoring\n",
    "- Making the model lightweight for practical deployment\n",
    "\n",
    "Traditional approaches:\n",
    "\n",
    "- Use heavy 3D CNN models\n",
    "- Require large computational resources\n",
    "- Not suitable for edge devices\n",
    "\n",
    "Our solution:\n",
    "A **Teacherâ€“Student Knowledge Distillation framework** where:\n",
    "\n",
    "- A powerful teacher model learns rich spatiotemporal features\n",
    "- A lightweight student model mimics it\n",
    "- Enables real-time anomaly detection\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q2: What dataset did you use?\n",
    "\n",
    "We used:\n",
    "\n",
    "- **UCF-Crime Dataset**\n",
    "\n",
    "Why:\n",
    "\n",
    "- Large-scale real-world surveillance dataset\n",
    "- Contains 13 types of anomalies\n",
    "- Standard benchmark for anomaly detection\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 2: ARCHITECTURE DESIGN\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q3: Explain your full system architecture.\n",
    "\n",
    "**Answer (Structured Explanation):**\n",
    "\n",
    "Our system consists of three major components:\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ Feature Extraction Module\n",
    "\n",
    "We use a 3D convolution-based spatiotemporal encoder.\n",
    "\n",
    "For example:\n",
    "\n",
    "- **I3D**\n",
    "- Or C3D-like architecture\n",
    "\n",
    "Purpose:\n",
    "\n",
    "- Extract spatial + temporal features simultaneously\n",
    "- Capture motion dynamics\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Teacher Model\n",
    "\n",
    "- Deep and computationally heavy\n",
    "- Trained on full feature representation\n",
    "- Produces anomaly scores\n",
    "- Acts as knowledge source\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ Student Model (Lightweight)\n",
    "\n",
    "- Smaller network\n",
    "- Learns to mimic teacher outputs\n",
    "- Uses distillation loss\n",
    "- Designed for deployment\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q4: Why use Knowledge Distillation?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Knowledge Distillation allows:\n",
    "\n",
    "- Transferring knowledge from large model â†’ small model\n",
    "- Preserving performance\n",
    "- Reducing model size\n",
    "- Enabling real-time processing\n",
    "\n",
    "Instead of training small model from scratch,\n",
    "we let it imitate:\n",
    "\n",
    "- Teacher feature representations\n",
    "- Teacher anomaly scores\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 3: TRAINING STRATEGY\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q5: What loss functions did you use?\n",
    "\n",
    "We used a combination of:\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ Classification / Anomaly Loss\n",
    "\n",
    "Binary cross-entropy between:\n",
    "\n",
    "- Predicted anomaly score\n",
    "- Ground truth label\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Distillation Loss\n",
    "\n",
    "Mean Squared Error (MSE):\n",
    "\n",
    "[\n",
    "L_{distill} = ||F_{teacher} - F_{student}||^2\n",
    "]\n",
    "\n",
    "This forces student features to align with teacher features.\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ Final Loss\n",
    "\n",
    "[\n",
    "L_{total} = \\lambda_1 L_{anomaly} + \\lambda_2 L_{distill}\n",
    "]\n",
    "\n",
    "Where Î» balances both objectives.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 4: TECHNICAL DECISIONS\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q6: Why use 3D CNN instead of 2D CNN?\n",
    "\n",
    "Because:\n",
    "\n",
    "2D CNN:\n",
    "\n",
    "- Processes frame independently\n",
    "- Cannot capture temporal dynamics\n",
    "\n",
    "3D CNN:\n",
    "\n",
    "- Operates on video clips\n",
    "- Captures motion information\n",
    "- Learns spatiotemporal patterns\n",
    "\n",
    "Anomalies are often motion-based (e.g., fighting, robbery).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q7: Why clip videos into segments?\n",
    "\n",
    "We segment videos into fixed-length clips because:\n",
    "\n",
    "- Allows batch processing\n",
    "- Maintains temporal consistency\n",
    "- Reduces memory overhead\n",
    "- Makes anomaly localization easier\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q8: How do you compute anomaly score?\n",
    "\n",
    "The anomaly score is computed as:\n",
    "\n",
    "- Output probability from final sigmoid layer\n",
    "- Higher value â†’ higher abnormality\n",
    "\n",
    "During testing:\n",
    "\n",
    "- Threshold applied\n",
    "- Score plotted over time\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 5: PERFORMANCE EVALUATION\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q9: What evaluation metric did you use?\n",
    "\n",
    "We used:\n",
    "\n",
    "- ROC-AUC\n",
    "- Frame-level AUC\n",
    "\n",
    "Why ROC-AUC?\n",
    "Because anomaly detection is highly imbalanced (normal >> abnormal).\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q10: What were your results?\n",
    "\n",
    "- Teacher Model AUC: ~85%\n",
    "- Student Model AUC: ~81%\n",
    "- Model size reduced significantly\n",
    "- Inference time improved\n",
    "\n",
    "This proves:\n",
    "Distillation preserved most performance while reducing complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 6: CHALLENGES\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q11: What challenges did you face?\n",
    "\n",
    "1. Class imbalance\n",
    "2. Long video duration\n",
    "3. Memory limitation during training\n",
    "4. Subtle anomalies hard to detect\n",
    "\n",
    "We solved imbalance using:\n",
    "\n",
    "- Balanced batch sampling\n",
    "- Weighted loss\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 7: COMPARISON WITH OTHER METHODS\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q12: How does your approach differ from traditional anomaly detection?\n",
    "\n",
    "Traditional methods:\n",
    "\n",
    "- Reconstruction-based (Autoencoder)\n",
    "- Predictive-based (Future frame prediction)\n",
    "\n",
    "Our method:\n",
    "\n",
    "- Feature learning + distillation\n",
    "- Designed for efficiency\n",
    "- Practical deployment-ready\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 8: LIMITATIONS\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q13: What are limitations of your project?\n",
    "\n",
    "- Requires teacher pre-training\n",
    "- Still needs labeled anomaly videos\n",
    "- Not fully zero-shot\n",
    "- Performance depends on teacher quality\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 9: DEPLOYMENT\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q14: How can this be deployed in real-world systems?\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Use lightweight student model\n",
    "2. Integrate with CCTV feed\n",
    "3. Process video in sliding window manner\n",
    "4. Trigger alert if anomaly score exceeds threshold\n",
    "\n",
    "Can be deployed on:\n",
    "\n",
    "- Edge GPU\n",
    "- Jetson device\n",
    "- Cloud server\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¯ SECTION 10: DEEP TECHNICAL QUESTIONS\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q15: Why does distillation improve small model performance?\n",
    "\n",
    "Because:\n",
    "\n",
    "- Teacher soft outputs contain dark knowledge\n",
    "- Encodes inter-class relationships\n",
    "- Provides smoother supervision\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q16: What happens if teacher is poorly trained?\n",
    "\n",
    "Student will:\n",
    "\n",
    "- Learn incorrect representations\n",
    "- Inherit teacherâ€™s bias\n",
    "- Performance degrades\n",
    "\n",
    "Teacher quality directly affects student.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Q17: How would you further improve this project?\n",
    "\n",
    "- Use transformer-based backbone\n",
    "- Add attention mechanism\n",
    "- Incorporate temporal consistency regularization\n",
    "- Apply self-supervised pretraining\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸŽ¤ FINAL VIVA STRATEGY\n",
    "\n",
    "For your defense:\n",
    "\n",
    "You must clearly explain:\n",
    "\n",
    "- 3D CNN concept\n",
    "- Knowledge Distillation intuition\n",
    "- Why lightweight model is necessary\n",
    "- How anomaly score is computed\n",
    "- Why ROC-AUC is important\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
