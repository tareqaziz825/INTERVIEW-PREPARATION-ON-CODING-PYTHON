Hash Table
==========

Definition:
-----------
A hash table (or hash map) is a data structure that stores key–value pairs. It uses a hash function to convert a key into an index in an array, allowing fast lookups, insertions, and deletions.

Key Concepts:
-------------
- Hash Function: Converts a key into an integer index.
- Buckets/Slots: Array positions where values are stored.
- Collisions: Occur when two keys hash to the same index.
- Collision Resolution: Techniques include chaining (linked lists) or open addressing (probing).
- Performance:
  - Average case: O(1) for search, insert, delete.
  - Worst case: O(n) if many collisions occur.

Hash Tables in Python:
----------------------
- Implemented using the built-in `dict` type.
- Keys: Any immutable object (strings, numbers, tuples).
- Hashing: Python uses the built-in `hash()` function to compute a hash code.
- Indexing: The hash code is mapped to a slot in an internal array.
- Collision Handling: Python uses open addressing with probing.
- Resizing: Python automatically resizes the table when it becomes too full.

Example in Python:
------------------
```python
# Creating a hash table using dict
students = {"Alice": 85, "Bob": 90, "Charlie": 78}

# Lookup
print(students["Bob"])   # 90

# Insert
students["David"] = 92

# Delete
del students["Alice"]

# Check existence
print("Charlie" in students)  # True
```

Comparison: General Hash Table vs Python Dictionary
---------------------------------------------------
- Key Type:
  - General Hash Table: Usually strings or numbers
  - Python dict: Any immutable object
- Collision Handling:
  - General Hash Table: Chaining or open addressing
  - Python dict: Open addressing with probing
- Resizing:
  - General Hash Table: Manual or automatic
  - Python dict: Automatic
- Complexity:
  - Both: Average O(1), Worst-case O(n)
- Extra Features:
  - General Hash Table: Basic mapping
  - Python dict: Iteration, comprehension, ordering (since Python 3.7)

Interview Prep Notes:
---------------------
- Understand hash functions, collisions, and load factor.
- Know that Python’s `dict` is a hash table using open addressing.
- Common interview questions:
  - Implement a hash table from scratch.
  - Explain collision resolution strategies.
  - Compare hash tables with arrays and linked lists.

---

Here’s a clean, text‑friendly version of **Collisions in Hash Map** that you can copy directly into a `.txt` file. I’ll maintain this format for all future topics so your notes stay consistent.

---

Collisions in Hash Table/Map
============================

Definition:
-----------
A collision in a hash map occurs when two different keys produce the same hash index. Since the hash table has a finite number of slots, collisions are inevitable when multiple keys are stored.

Why Collisions Happen:
----------------------
- Finite array size: Limited slots in the table.
- Hash function limits: Different keys can map to the same index.
- Example: In a table of size 10, both "cat" and "dog" might hash to index 3.

Collision Resolution Techniques:
--------------------------------

1. Chaining
-----------
- Each slot stores a linked list (or another structure) of key–value pairs.
- Multiple keys that hash to the same index are chained together.
- Advantages: Easy to implement, handles unlimited collisions.
- Disadvantages: Extra memory for pointers, slower lookups if chains grow long.

Example (conceptual in Python):
```python
hash_table = [[] for _ in range(10)]

def insert(key, value):
    index = hash(key) % 10
    hash_table[index].append((key, value))
```

2. Open Addressing
------------------
- Collisions are resolved by finding another empty slot in the array.
- Methods:
  - Linear probing: Check the next slot sequentially.
  - Quadratic probing: Check slots at increasing intervals (1, 4, 9…).
  - Double hashing: Use a second hash function to calculate step size.
- Advantages: No extra memory for linked lists.
- Disadvantages: Performance degrades as the table fills up.

Note: Python’s `dict` uses open addressing with probing.

Comparison of Collision Handling:
---------------------------------
- Chaining:
  - Pros: Simple, unlimited collisions.
  - Cons: Extra memory, slower if chains grow.
- Linear Probing:
  - Pros: Simple, cache-friendly.
  - Cons: Clustering problem.
- Quadratic Probing:
  - Pros: Reduces clustering.
  - Cons: More complex, may still fail.
- Double Hashing:
  - Pros: Better distribution.
  - Cons: More computation.

Interview Prep Notes:
---------------------
- Be ready to explain collisions clearly: “Collisions happen when two keys hash to the same index.”
- Know both strategies: chaining vs open addressing.
- Python specifics: `dict` uses open addressing with probing.
- Common interview questions:
  - Implement collision handling in a custom hash table.
  - Compare performance of chaining vs open addressing.
  - Explain how load factor affects collisions.

---

Here’s a clean, text‑friendly explanation of **Chaining, Open Addressing, and Linear Probing** that you can copy directly into a `.txt` file for your notes.  

---

Chaining, Open Addressing, and Linear Probing
=============================================

Chaining
--------
Definition:
- Chaining is a collision resolution technique where each slot in the hash table stores a linked list (or another data structure) of key–value pairs.
- If multiple keys hash to the same index, they are stored together in that slot.

Advantages:
- Simple to implement.
- Can handle unlimited collisions (limited only by memory).

Disadvantages:
- Requires extra memory for pointers.
- Lookup time increases if chains grow long.

Example (conceptual in Python):
```python
hash_table = [[] for _ in range(10)]

def insert(key, value):
    index = hash(key) % 10
    hash_table[index].append((key, value))
```

---

Open Addressing
---------------
Definition:
- Open addressing resolves collisions by finding another empty slot in the table instead of storing multiple items in the same slot.
- When a collision occurs, the algorithm probes the table to find the next available position.

Advantages:
- No extra memory needed for linked lists.
- Cache-friendly since data is stored in a single array.

Disadvantages:
- Performance degrades as the table fills up (high load factor).
- Requires careful probing strategy to avoid clustering.

Methods:
- Linear Probing
- Quadratic Probing
- Double Hashing

Note: Python’s `dict` uses open addressing with probing.

---

Linear Probing
--------------
Definition:
- Linear probing is a specific type of open addressing.
- When a collision occurs, the algorithm checks the next slot sequentially until an empty slot is found.

Advantages:
- Simple to implement.
- Cache-friendly due to sequential memory access.

Disadvantages:
- Clustering problem: consecutive slots may fill up, leading to longer probe sequences.
- Performance decreases as load factor increases.

Example (conceptual in Python):
```python
table_size = 10
hash_table = [None] * table_size

def insert(key, value):
    index = hash(key) % table_size
    while hash_table[index] is not None:
        index = (index + 1) % table_size  # linear probing
    hash_table[index] = (key, value)
```

---

Comparison
----------
- Chaining:
  - Stores multiple items in the same slot using linked lists.
  - Handles unlimited collisions but requires extra memory.
- Open Addressing:
  - Stores items directly in the table by probing for empty slots.
  - More memory-efficient but performance depends on load factor.
- Linear Probing:
  - A simple form of open addressing.
  - Easy to implement but suffers from clustering.

---

Interview Prep Notes
--------------------
- Be ready to explain the difference between chaining and open addressing.
- Know that Python’s `dict` uses open addressing with probing.
- Understand the clustering problem in linear probing.
- Common interview questions:
  - Implement a hash table with chaining.
  - Implement a hash table with open addressing (linear probing).
  - Compare performance of chaining vs open addressing.
  - Explain how load factor affects collisions and probing efficiency.

---

Here’s a simple, text‑friendly explanation of **ArrayList and amortized insertion runtime** that you can copy directly into a `.txt` file for your notes.  

---

ArrayList and Amortized Insertion
=================================

Definition:
-----------
- An ArrayList (in Java) or dynamic array (in other languages) is an array-like data structure that resizes itself automatically when it becomes full.
- It provides **O(1)** average-time access to elements by index.
- When the array is full, it **doubles its size** (resizing factor = 2 in Java).

How Resizing Works:
-------------------
1. Start with an array of some capacity (say 4).
2. When you try to insert the 5th element, the array is full.
3. A new array of double the size (capacity = 8) is created.
4. All existing elements are copied into the new array.
5. The new element is added.

Why Amortized Insertion is O(1):
--------------------------------
- **Normal case**: Adding an element just places it in the next empty slot → O(1).
- **Resize case**: Occasionally, when the array is full, resizing happens → O(n) because all elements must be copied.
- But resizing does not happen every time. It happens rarely (only when capacity is exceeded).
- Over many insertions, the cost of resizing is spread out ("amortized") across all insertions.

Example of Copying Work:
------------------------
Suppose the final array size is `n`.  
- Last resize: copied n/2 elements.  
- Previous resize: copied n/4 elements.  
- Previous resize: copied n/8 elements.  
- … and so on, until the first resize (1 element copied).  

Total work = n/2 + n/4 + n/8 + … + 1 ≈ n.  
So across n insertions, the total extra work is O(n).  
That means the average cost per insertion = O(n)/n = O(1).

Key Points:
-----------
- **Access by index**: O(1).  
- **Insertion (amortized)**: O(1).  
- **Insertion (worst case, when resizing)**: O(n).  
- **Resizing factor**: Usually 2 in Java, but can vary in other languages.  
- **Memory trade-off**: Extra unused capacity after resizing.  

Interview Prep Notes:
---------------------
- Be ready to explain why ArrayList insertion is amortized O(1).  
- Know the difference between **amortized O(1)** and **worst-case O(n)**.  
- Understand how doubling capacity ensures efficiency.  
- Common interview questions:
  - Implement a dynamic array from scratch.
  - Explain amortized analysis.
  - Compare ArrayList with LinkedList (ArrayList has O(1) access, LinkedList has O(1) insertion at ends).

---

StringBuilder Concept
=====================

Definition:
-----------
- In Java, `StringBuilder` is used to efficiently build strings by appending parts together.  
- Strings in Java are immutable (cannot be changed once created).  
- If you keep concatenating strings with `+`, Java creates new string objects each time, which is slow.  
- `StringBuilder` avoids this problem by using a resizable array internally and only creating the final string when needed.

How It Works:
-------------
- You create a `StringBuilder` object.  
- Append strings to it using `.append()`.  
- At the end, call `.toString()` to get the final combined string.  

Example in Java:
----------------
```java
String joinWords(String[] words) {
    StringBuilder sentence = new StringBuilder();
    for (String w : words) {
        sentence.append(w);
    }
    return sentence.toString();
}
```

---

Python Equivalent
=================

In Python, strings are also immutable. But Python provides efficient ways to join strings without repeatedly creating new objects.

1. Using `"".join()` (Most Efficient)
-------------------------------------
```python
def join_words(words):
    sentence = "".join(words)
    return sentence

print(join_words(["Hello", " ", "World", "!"]))
# Output: Hello World!
```

Explanation:
- `"".join(words)` takes all strings in the list and joins them together efficiently.  
- This is the Pythonic equivalent of using `StringBuilder` in Java.  

2. Using a List and `append()` (Similar to StringBuilder)
---------------------------------------------------------
```python
def join_words(words):
    sentence_parts = []
    for w in words:
        sentence_parts.append(w)
    return "".join(sentence_parts)

print(join_words(["Python", " ", "is", " ", "fast"]))
# Output: Python is fast
```

Explanation:
- `sentence_parts` is a list that collects all pieces.  
- At the end, `"".join(sentence_parts)` combines them into one string.  
- This avoids repeated string concatenation.  

---

Key Points
----------
- **Java**: Use `StringBuilder` to avoid costly string concatenations.  
- **Python**: Use `"".join(list_of_strings)` for efficient string building.  
- Both approaches rely on collecting pieces first, then combining them once.  

Interview Prep Notes
--------------------
- Be ready to explain why string concatenation can be inefficient (immutability).  
- Know that `StringBuilder` in Java and `"".join()` in Python solve this problem.  
- Common interview questions:
  - Implement a function that joins words efficiently.  
  - Explain the difference between repeated concatenation vs using a builder/join.  
  - Compare performance of `"".join()` vs `+` concatenation in Python.  

---



